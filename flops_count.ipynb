{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from tests.networks.utils import Generator\n",
    "\n",
    "from experiments.atari.utils import generate_keys\n",
    "from idqn.environments.atari import AtariEnv\n",
    "from idqn.sample_collection.replay_buffer import ReplayBuffer\n",
    "from idqn.networks.architectures.dqn import AtariDQN\n",
    "from idqn.networks.architectures.iqn import AtariIQN\n",
    "from idqn.networks.architectures.idqn import AtariiDQN\n",
    "from idqn.networks.architectures.iiqn import AtariiIQN\n",
    "from idqn.utils.head_behaviorial_policy import head_behaviorial_policy\n",
    "\n",
    "q_key, train_key = generate_keys(1)\n",
    "\n",
    "env = AtariEnv(\"Breakout\")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    (env.state_height, env.state_width),\n",
    "    1000000,\n",
    "    32,\n",
    "    1,\n",
    "    0.99,\n",
    "    lambda x: np.clip(x, -1, 1),\n",
    ")\n",
    "\n",
    "sample_generator = Generator(32, (env.state_height, env.state_width, env.n_stacked_frames)) \n",
    "\n",
    "q_dqn = AtariDQN(\n",
    "    (env.state_height, env.state_width, env.n_stacked_frames),\n",
    "    env.n_actions,\n",
    "    math.pow(0.99, 1),\n",
    "    q_key,\n",
    "    0.001,\n",
    "    0.001,\n",
    "    4,\n",
    "    6000,\n",
    ")\n",
    "\n",
    "q_iqn = AtariIQN(\n",
    "    (env.state_height, env.state_width, env.n_stacked_frames),\n",
    "    env.n_actions,\n",
    "    math.pow(0.99, 1),\n",
    "    q_key,\n",
    "    0.001,\n",
    "    0.001,\n",
    "    4,\n",
    "    6000,\n",
    ")\n",
    "\n",
    "q_idqn = AtariiDQN(\n",
    "    5 + 1,\n",
    "    (env.state_height, env.state_width, env.n_stacked_frames),\n",
    "    env.n_actions,\n",
    "    math.pow(0.99, 1),\n",
    "    q_key,\n",
    "    head_behaviorial_policy(\"uniform\", 5 + 1),\n",
    "    0.001,\n",
    "    0.001,\n",
    "    4,\n",
    "    30,\n",
    "    6000,\n",
    "    True,\n",
    ")\n",
    "\n",
    "q_iiqn = AtariiIQN(\n",
    "    3 + 1,\n",
    "    (env.state_height, env.state_width, env.n_stacked_frames),\n",
    "    env.n_actions,\n",
    "    math.pow(0.99, 1),\n",
    "    q_key,\n",
    "    head_behaviorial_policy(\"uniform\", 3 + 1),\n",
    "    0.001,\n",
    "    0.001,\n",
    "    4,\n",
    "    30,\n",
    "    6000,\n",
    "    32,\n",
    "    64, \n",
    "    64,\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flops(q):\n",
    "    best_action_compiled = jax.jit(q.best_action, static_argnames=\"self\").lower(q.params, sample_generator.generate_state(jax.random.PRNGKey(0)), jax.random.PRNGKey(0)).compile()\n",
    "    learn_on_batch_compiled = jax.jit(q.learn_on_batch, static_argnames=\"self\").lower(q.params, q.target_params, q.optimizer_state, sample_generator.generate_samples(jax.random.PRNGKey(0)), jax.random.PRNGKey(0)).compile()\n",
    "\n",
    "    return best_action_compiled, learn_on_batch_compiled\n",
    "\n",
    "\n",
    "print(\"DQN\")\n",
    "dqn_best_action_compiled, dqn_learn_on_batch_compiled = count_flops(q_dqn)\n",
    "print(\"FLOPs best action: \", dqn_best_action_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"FLOPs to learn on a batch: \", dqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"i-DQN\")\n",
    "idqn_best_action_compiled, idqn_learn_on_batch_compiled = count_flops(q_idqn)\n",
    "print(\"FLOPs best action: \", idqn_best_action_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"FLOPs to learn on batch: \", idqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"DQN vs i-DQN\")\n",
    "print(\"Best action FLOPs ratio:\", idqn_best_action_compiled.cost_analysis()[0][\"flops\"] / dqn_best_action_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"Learn on batch FLOPs ratio:\", idqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"] / dqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"IQN\")\n",
    "iqn_best_action_compiled, iqn_learn_on_batch_compiled = count_flops(q_iqn)\n",
    "print(\"FLOPs best action: \", iqn_best_action_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"FLOPs to learn on a batch: \", iqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"i-IQN\")\n",
    "iiqn_best_action_compiled, iiqn_learn_on_batch_compiled = count_flops(q_iiqn)\n",
    "print(\"FLOPs best action: \", iiqn_best_action_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"FLOPs to learn on a batch: \", iiqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"IQN vs i-IQN\")\n",
    "print(\"Best action FLOPs ratio:\", iiqn_best_action_compiled.cost_analysis()[0][\"flops\"] / iqn_best_action_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"Learn on batch FLOPs ratio:\", iiqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"] / iqn_learn_on_batch_compiled.cost_analysis()[0][\"flops\"])\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
